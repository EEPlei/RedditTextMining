---
title: 'Homework 5 Duke Dinosaurs'
author: "Lei Qian"
date: "December 6, 2015"
output: html_document
---
```{r}
source("reddit_rhipe.R")
source("utc_rhipe.R")
source("task3_rhipe.R")

```

##Task 1##
```{r}

library(dplyr)
library(magrittr)

load("Jan.Rdata")
load("Feb.Rdata")
load("Mar.Rdata")
load("Apr.Rdata")
load("May.Rdata")

mnth <- list(Jan = Jan, Feb = Feb, Mar = Mar, Apr = Apr, May = May)

example <- function(x){
  x25 <- x[1:25,]
  return(x25)
}

bb <- function(x,y){
  x25 <- x[1:25,]
  # top 25 subreddit for x month
  x25y <- y %>% filter(key %in% x25$key)
  # x top 25 subreddit from month y 
  x25y <- x25y[match(x25$key, x25y$key),]
  # reorder y to match x 
  per.change <- (x25$value - x25y$value)/x25y$value
  per.change <- sprintf("%1.2f%%", 100*per.change)
  x25$PercentageChange <- per.change
  # percent change from y to x 
  colnames(x25) <- c("SubReddit", "Frequency", "%Change")
  return(x25)
}

billboard <- list(January = Jan[1:25,], 
                  February = bb(Feb, Jan), 
                  March = bb(Mar, Feb), 
                  April = bb(Apr, Mar),
                  May = bb(May, Apr))


```
For Task 1, we went through the months January, February, March, April, and May to analyze the category subreddits. Our MapReduce, counted the number of times a comment belonged to a subreddit. We then, sorted out the top 25 most popular subreddits for each month and compared it to its ranking from the previous month. Since we do not have data for December, we are unable to calculate how many of the top 25 subreddits in January has changed in ranking. 
```{r}
billboard[[1]]
```
This will give us the top 25 subreddits in January. The 'key' column gives us the top 25 subreddit names and the 'value' column tells us how many comments belonged to these subreddits.         
`             key   value`   
`       AskReddit 4712795`   
`             nfl  932460`   
`           funny  930098`   
` leagueoflegends  904297`   
`            pics  778942`   
`       worldnews  670872`   
`   todayilearned  599295`   
`  DestinyTheGame  587774`   
`   AdviceAnimals  577463`   
`          videos  570938`   
`             nba  513906`   
`    pcmasterrace  512582`   
`            news  441657`   
`   SquaredCircle  427546`   
`             CFB  400464`   
`          hockey  399466`   
`             WTF  391393`   
`          gaming  383858`   
`          soccer  378902`   
`           DotA2  374962`   
`          movies  370386`   
` GlobalOffensive  348153`   
`   relationships  252429`   
`            gifs  243083`   
`           anime  239394`   
The billboard for the other four months will look something like this. 
```{r}
billboard[[4]]
```
          
`         SubReddit Frequency %Change`   
`         AskReddit   4158888   1.18%`   
`   leagueoflegends    999977  -4.20%`   
`             funny    755065  -7.60%`   
`               nba    717799  42.65%`   
`            hockey    672511  34.18%`   
`              pics    660970  -6.49%`     
`            videos    594071   7.85%`   
`              news    569881  20.79%`   
`         thebutton    520686     NA%`   
`      pcmasterrace    517143  16.97%`   
`     todayilearned    514287  -7.84%`   
`         worldnews    496264 -13.04%`   
`     AdviceAnimals    473058  -3.63%`   
`            gaming    447396  17.16%`   
`            soccer    417589  -5.82%`   
`               nfl    405382 -38.26%`   
`               WTF    397123  -3.90%`   
`            movies    388167  20.38%`   
`             DotA2    380744   1.68%`   
`     SquaredCircle    366378 -32.06%`   
`   GlobalOffensive    357393  14.15%`   
`    DestinyTheGame    349363  -3.89%`   
`            amiibo    297705  70.58%`   
`     fatpeoplehate    292376   5.46%`   
`CasualConversation    279950  41.84%`   

Out of all five months, April is the only month where one of the subreddits in the top 25 does not exist for the previous month, which is March. We know this because the %Change column is NA%. We calculate this column by searching for the number of comments in a subreddit for a previous month which we will call X. The number of comments for a subreddit in the current month is Y. To find the percentage change we use this formula: (Y-X)/X. In the case of CasualConservation, there are 279950 posts in this subreddit category for April. It is a 41.84% increase in posts for this subreddit from March.   

##Task 2##
For task 2, we needed to create plots that displayed the frequency of reddits that were aggregated to an hourly level. We first needed to aggregate the time that the comments were made. We approached this from a MapReduce methodology based on Rhipe and Hadoop. We first filtered for the utc time stamps. 
```{r}
  time_converter = function(time){
    #below code gives me the numeric version of each utcs
    newtime = lapply(time, as.numeric)
    #below code applies as.POSIXct function to the newtime dataframe
    stdtime = lapply(newtime, as.POSIXct,origin = '1970-01-01')
    #below code replaces the minutes and seconds with 00:00 so its easy to aggregate
    stacked_time = lapply(stdtime, str_replace,":[0-5][0-9]:[0-5][0-9]", ":00:00")
    #below codes converts back the time to utc

    back_to_utc = lapply(stacked_time, as.POSIXct)
    converted_utc = sapply(back_to_utc, as.numeric)
    
    
    return(converted_utc)
  }
```
Specifically, this function converts the filtered UTC time stamp into a standard time format--year/month/date Hour:Minute:Second--s and replaces it so that it is aggregated on an hourly level. For example, if there is a UTC time stamp that reads `1421739942` then it is converted into a standard time that reads `2015-01-20 02:45:42 EST`. Then to aggregate to an hourly level, we replace the Minute:Second timekeys--which correspond to `45:42` in the previous example--to `00:00`. Thus the converted time stamp can be read as `2015-01-20 02:00:00 EST`. Then this time is transformed again into a UTC timestamp. 

The second part to task2 was running the same process for comments that were "gilded". For a non-reddit user, the concept of being "gilded" would be unfamiliar. Here is a simple analogy--gilded:reddit=likes:facebook. Thus, "gilded" to a reddit post is similar to a "like" on facebook. One twist to the plot is that if you are to "gild" a post, you would have to pay money to reddit. This twist makes "gilded" posts on reddit extremely rare, which is the reason that most of the "gilded" section in the short and long json files are 0. We used a function called "gold" that would specifically extract and count the gilded posts. 

```{r}
  gold <- lapply(
    seq_along(map.keys), 
    function(r) 
    {
      gilded = fromJSON(map.values[[r]])$gilded
    }
  ) 
  gindex <- lapply(gold, as.numeric)
  gindex <- which(gindex == 1)
  map.keys <- map.keys[gindex]
  lapply(
    seq_along(map.keys), 
    function(r) 
    {
      time = fromJSON(map.values[[r]])$created_utc
      key = time_converter(time)
      value = 1
      lapply(key, rhcollect, value=1)
    }
  )
```

Through our "gold" function, we first define our mapvalues as "gilded", namely using "gilded" as the key. When we select the gilded parts from each line of the json file, we transformed the value as a numeric item and filtered for the values that were equal to 1. Thus, we were able to disregard comments that were ungilded and select only the gilded comments. After the filteration, we applied the time_converter function to aggregate the UTCs on an hourly level. 

```{r}
source("utc_rhipe.R")
load("jan_utc.Rdata")
load("jan_gild.Rdata")
jan_utc1 <- jan_utc[order(jan_utc$key),]
plot(x = jan_utc1$key, y = jan_utc1$value)
# plot from December 31, 2014 at 7 p.m. to January 31, 2015 at 6 p.m. # 
# plots for every hour # 

new.key <- as.POSIXct(jan_utc$key, origin = '1970-01-01')
new.time <- sapply(new.key, str_extract, "[0-9][0-9]:")
new.time <- as.numeric(sapply(new.time, str_extract, "[0-9][0-9]"))
new.t <- new.time[-which(is.na(new.time))]
new.df <- jan_utc[-which(is.na(new.time)),]
new.df <- cbind(hour = new.t, new.df)
new.df <- new.df[,-2]
time.ind <- unique(new.t)[order(unique(new.t))]
time.sum <- function(time.i){
  new.sum <- sum(new.df[new.df$hour == time.i, ]$value)
  return(new.sum)
}
time.new <- sapply(time.ind, time.sum)
plot(x = time.ind, y = time.new)
# plots out number of comments from 1:00 am to midnight #

jan_gild1 <- jan_gilded[order(jan_gilded$key),]
plot(x = jan_gild1$key, y = jan_gild1$value)
# plot from December 31, 2014 at 7 p.m. to January 31, 2015 at 6 p.m. # 
# plots for every hour # 


new.key <- as.POSIXct(jan_gilded$key, origin = '1970-01-01')
new.time <- sapply(new.key, str_extract, "[0-9][0-9]:")
new.time <- as.numeric(sapply(new.time, str_extract, "[0-9][0-9]"))
new.t <- new.time[-which(is.na(new.time))]
new.df <- jan_gilded[-which(is.na(new.time)),]
new.df <- cbind(hour = new.t, new.df)
new.df <- new.df[,-2]
time.ind <- unique(new.t)[order(unique(new.t))]
time.sum <- function(time.i){
  new.sum <- sum(new.df[new.df$hour == time.i, ]$value)
  return(new.sum)
}
time.new <- sapply(time.ind, time.sum)
plot(x = time.ind, y = time.new)
# plots out number of comments from 1:00 am to midnight #
```
It seems that in general, comments are lowest at 5 in the morning and highest at 3 pm. In fact, from 10 to midnight, comments are fairly high. It seems that most redditors are night-owls. This holds true even for gilded comments. 
##Task 3##
For task 3, we need to find out what redditors are saying on Valentine's Day. To do that, we pick not only Feburary 14, but also January 14 and March 14 as control days. Basically the idea is still based on MapReduce method on Rhipe and Hadoop. Similarly, we have wc_reduce, wc_map and MapReduce functions. But we need to filter out 1/14, 2/14 and 3/14 and perform word counting only for these three days. wc_reduce and MapReduce part are pretty much the same. wc_map part is as follows.
```{r}

wc_map = expression({
  suppressMessages(library(stringr))
  suppressMessages(library(jsonlite))
  suppressMessages(library(NLP))
  suppressMessages(library(tm))
  x <- stopwords("en")
  y <- stopwords("SMART")
  sw <- union(x,y)
  lapply(
    seq_along(map.keys), 
    function(r) 
    {
      time = fromJSON(map.values[[r]])$created_utc
      new.time = as.POSIXct(as.numeric(time), origin='1970-01-01')
      strs = strsplit(toString(new.time), " ")
      strs2 = strsplit(strs[[1]][1], "-")
      date = strs2[[1]][3]
      if(date == "14"){
        line = tolower(fromJSON(map.values[[r]])$body)
        line = gsub("[-—]"," ",line)
        line = gsub("[^'`’[:alpha:][:space:]]","",line,perl=TRUE)
        line = gsub("(^\\s+|\\s+$)","",line)
        line = strsplit(line, "\\s+")[[1]]
        line = line[line != ""]
        line <- setdiff(line, sw)
        lapply(line, rhcollect, value=1)
      }
    }     
  )
})

```
In wc_map, we use stopwords function from tm package to remove stop words.
```{r}
library(tm)
  x <- stopwords("en")
  y <- stopwords("SMART")
  sw <- union(x,y)
```
x contains 174 stop words. y contains 571 stop words. sw consists of x and y. The stop words are as follows.

```{r}
sw[1:30]
```

`[1] "i" "me" "my" "myself" "we" "our" "ours" "ourselves" "you"`          
`[10] "your" "yours" "yourself" "yourselves" "he" "him" "his" "himself" "she"`     
`[19] "her" "hers" "herself" "it" "its" "itself" "they" "them" "their"`           
`[28] "theirs" "themselves" "what"`         
```{r}
tail(sw)
```

`[1] "x" "y" "yes" "yet" "z" "zero"`    
```{r}
sw[550:580]   

```

`[1] "wants" "way" "welcome" "well" "went" "whatever" "whence" "whenever" "whereafter"` `[10] "whereas" "whereby" "wherein" "whereupon" "wherever" `    
`"whether" "whither" "whoever" "whole"`    
`[19] "whose" "will" "willing" "wish" "within" "without" "wonder" "x" "y" `         
`[28]   "yes" "yet" "z" "zero"`   


In the function inside lapply, we use as.POSIXct() function to transfer created_utc to normal date. Then use strsplit to extract the specific day of the date. Since we only perform this function on RC_2015-01.json, RC_2015-02.json and RC_2015-03.json, if the day is 14 then that is what we want. In the if clause, we only care about body, let line = tolower(fromJSON(map.values[[r]])$body). Then we strip things like punctuation and capitalization on line and use setdiff() to remove stop words we previously defined. Lastly, we use rhcollect to assign the value 1 to each 
line.

We perform MapReduce on RC_2015-01.json, RC_2015-02.json and RC_2015-03.json and then save the results as Jan_14.Rdata, Feb_14.Rdata and Mar_14.Rdata.

From the results, we found top 120 words from the three days are pretty much similiar. But when we slice 120-130 rows from the three days, things are a bit different. 

```{r}
load("Jan_14.Rdata")
load("Feb_14.Rdata")
load("Mar_14.Rdata")

```


  
`> Jan_14 %>% slice(120:130)`   
`          key value`   
`1        side 14422`   
`2       video 14267`   
`3     working 14046`   
`4  completely 14010`   
`5        easy 13952`   
`6      reddit 13913`   
`7      system 13870`   
`8      couple 13744`   
`9        link 13709`   
`10 experience 13692`   
`11     friend 13577`   
`> Feb_14 %>% slice(120:130)`   
`       key value`   
`1       op 10520`   
`2   action 10499`   
`3  contact 10479`   
`4     home 10477`   
`5    worth 10451`   
`6    works 10417`   
`7    sense 10407`   
`8     damn 10348`   
`9    today 10208`   
`10  single 10152`   
`11   story 10071`   
`> Mar_14 %>% slice(120:130)`   
`          key value`   
`1       small 10807`   
`2          op 10798`   
`3       worth 10797`   
`4        half 10643`   
`5      school 10632`   
`6        home 10591`   
`7       story 10589`   
`8  completely 10561`   
`9     players 10515`   
`10     called 10281`   
`11       hate 10280`   

As we can see above, there are some frequently words are like "damn", "today", "single" on Valentines Day which are not in two other days. Seems like there might be more bachelors positng on reddit complaining about their singleness on Valentines Day.  
  
