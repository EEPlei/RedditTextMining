---
title: 'Homework 5 Duke Dinosaurs'
author: "Lei Qian"
date: "December 6, 2015"
output: html_document
---
```{r}
source("reddit_rhipe.R")
source("utc_rhipe.R")
source("task3_rhipe.R")

```

Task 1
```{r}

library(dplyr)
library(magrittr)

load("Jan.Rdata")
load("Feb.Rdata")
load("Mar.Rdata")
load("Apr.Rdata")
load("May.Rdata")

mnth <- list(Jan = Jan, Feb = Feb, Mar = Mar, Apr = Apr, May = May)

example <- function(x){
  x25 <- x[1:25,]
  return(x25)
}

bb <- function(x,y){
  x25 <- x[1:25,]
  # top 25 subreddit for x month
  x25y <- y %>% filter(key %in% x25$key)
  # x top 25 subreddit from month y 
  x25y <- x25y[match(x25$key, x25y$key),]
  # reorder y to match x 
  per.change <- (x25$value - x25y$value)/x25y$value
  per.change <- sprintf("%1.2f%%", 100*per.change)
  x25$PercentageChange <- per.change
  # percent change from y to x 
  colnames(x25) <- c("SubReddit", "Frequency", "%Change")
  return(x25)
}

billboard <- list(January = Jan[1:25,], 
                  February = bb(Feb, Jan), 
                  March = bb(Mar, Feb), 
                  April = bb(Apr, Mar),
                  May = bb(May, Apr))


```

Task 2
```{r}

```
Task 3
```{r}

For task 3, we need to find out what redditors are saying on Valentines Day. To do that, we pick not only Feburary 
14, but also January 14 and March 14 as control days. Basically the idea is still based on MapReduce method on Rhipe 
and Haddop. Similarly, we have wc_reduce, wc_map and MapReduce functions. But we need to filter out 1/14, 2/14 and 
3/14 and perform word counting only for these three days. wc_reduce and MapReduce part are pretty much the same. 
wc_map part is as follows.

wc_map = expression({
  suppressMessages(library(stringr))
  suppressMessages(library(jsonlite))
  suppressMessages(library(NLP))
  suppressMessages(library(tm))
  x <- stopwords("en")
  y <- stopwords("SMART")
  sw <- union(x,y)
  lapply(
    seq_along(map.keys), 
    function(r) 
    {
      time = fromJSON(map.values[[r]])$created_utc
      new.time = as.POSIXct(as.numeric(time), origin='1970-01-01')
      strs = strsplit(toString(new.time), " ")
      strs2 = strsplit(strs[[1]][1], "-")
      date = strs2[[1]][3]
      if(date == "14"){
        line = tolower(fromJSON(map.values[[r]])$body)
        line = gsub("[-—]"," ",line)
        line = gsub("[^'`’[:alpha:][:space:]]","",line,perl=TRUE)
        line = gsub("(^\\s+|\\s+$)","",line)
        line = strsplit(line, "\\s+")[[1]]
        line = line[line != ""]
        line <- setdiff(line, sw)
        lapply(line, rhcollect, value=1)
      }
    }     
  )
})

In wc_map, we use stopwords function from tm package to remove stop words.

  library(tm)
  x <- stopwords("en")
  y <- stopwords("SMART")
  sw <- union(x,y)
  
x contains 174 stop words. y contains 571 stop words. sw consists of x and y. The stop words are as follows.

> sw[1:30]
 [1] "i" "me" "my" "myself" "we" "our" "ours" "ourselves" "you"       
[10] "your" "yours" "yourself" "yourselves" "he" "him" "his" "himself" "she"  [19] "her" "hers" "herself" "it" "its" 
"itself" "they" "them" "their"     
[28] "theirs" "themselves" "what"      
> tail(sw)
 [1] "x" "y" "yes" "yet" "z" "zero"
> sw[550:580]
 [1] "wants" "way" "welcome" "well" "went" "whatever" "whence" "whenever" "whereafter"
[10] "whereas" "whereby" "wherein" "whereupon" "wherever" "whether" "whither" "whoever" "whole"     
[19] "whose" "will" "willing" "wish" "within" "without" "wonder" "x" "y"      [28] "yes" "yet" "z" "zero"

In the function inside lapply, we use as.POSIXct() function to transfer created_utc to normal date. Then use strsplit 
to extract the specific day of the date. Since we only perform this function on RC_2015-01.json, RC_2015-02.json and 
RC_2015-03.json, if the day is 14 then that is what we want. In the if clause, we only care about body, 
let line = tolower(fromJSON(map.values[[r]])$body). Then we strip things like punctuation and capitalization on line 
and use setdiff() to remove stop words we previously defined. Lastly, we use rhcollect to assign the value 1 to each 
line.

We perform MapReduce on RC_2015-01.json, RC_2015-02.json and RC_2015-03.json and then save the results as Jan_14.Rdata, 
Feb_14.Rdata and Mar_14.Rdata.

From the results, we found top 120 words from the three days are pretty much similiar. But when we slice 120-130 rows 
from the three days, things are a bit different. 

> Jan_14 %>% slice(120:130)
          key value
1        side 14422
2       video 14267
3     working 14046
4  completely 14010
5        easy 13952
6      reddit 13913
7      system 13870
8      couple 13744
9        link 13709
10 experience 13692
11     friend 13577
> Feb_14 %>% slice(120:130)
       key value
1       op 10520
2   action 10499
3  contact 10479
4     home 10477
5    worth 10451
6    works 10417
7    sense 10407
8     damn 10348
9    today 10208
10  single 10152
11   story 10071
> Mar_14 %>% slice(120:130)
          key value
1       small 10807
2          op 10798
3       worth 10797
4        half 10643
5      school 10632
6        home 10591
7       story 10589
8  completely 10561
9     players 10515
10     called 10281
11       hate 10280

As we can see above, there are some frequently words are like "damn", "today", "single" on Valentines Day which are not 
in two other days. Seems like there might be more bachelors positng on reddit complaining about their singleness on 
Valentines Day.  

```
